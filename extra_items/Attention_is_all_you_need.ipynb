{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T12:50:45.861075Z",
     "start_time": "2025-05-09T12:50:45.855655Z"
    }
   },
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "# Model dimension\n",
    "dim_model = 64\n",
    "\n",
    "# Sequence length\n",
    "seq_length = 10\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = 100"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:51:25.919972Z",
     "start_time": "2025-05-09T12:51:25.910737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the function to create an embedding matrix\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "\n",
    "    # Create an embedding matrix where each row represents a vocabulary token\n",
    "    # The array is initialized with normally distributed random values\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "\n",
    "    # For each token index in the input, select the corresponding embedding from the array\n",
    "    # Returns an array of embeddings corresponding to the input sequence\n",
    "    return np.array([embed[i] for i in input])"
   ],
   "id": "7e565f1ece0019b1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:50:48.795437Z",
     "start_time": "2025-05-09T12:50:48.786688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Softmax Activation Function\n",
    "def softmax(x):\n",
    "\n",
    "    # Calculates the exponential of each input element, adjusted by the maximum value in the input\n",
    "    # to avoid numeric overflow\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "    # Divide each exponential by the sum of the exponentials along the last axis (axis=-1)\n",
    "    # Reshape(-1, 1) ensures that division is performed correctly in a multidimensional context\n",
    "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
   ],
   "id": "ab20f4dd6e596132",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:50:50.782704Z",
     "start_time": "2025-05-09T12:50:50.774738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the function to calculate attention scaled by dot product\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "\n",
    "    # Calculate the dot product between Q and the transpose of K\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "\n",
    "    # Gets the dimension of the key vectors\n",
    "    depth = K.shape[-1]\n",
    "\n",
    "    # Scale the logits by dividing them by the square root of the depth\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "    # Apply the softmax function to obtain the attention weights\n",
    "    attention_weights = softmax(logits)\n",
    "\n",
    "    # Multiply the attention weights by the V values to get the final output\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "    # Returns the weighted output\n",
    "    return output"
   ],
   "id": "fb3034cac9078450",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:50:56.251016Z",
     "start_time": "2025-05-09T12:50:56.242948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defines the function that applies a linear transformation followed by softmax\n",
    "def linear_and_softmax(input):\n",
    "\n",
    "    # Initialize a weight matrix with normally distributed random values\n",
    "    # This matrix connects each model dimension (dim_model) to each vocabulary word (vocab_size)\n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "\n",
    "    # Performs the linear operation (scalar product) between the input and the weight matrix\n",
    "    # The result, logits, is a vector that represents the input transformed into a higher-dimensional space\n",
    "    logits = np.dot(input, weights)\n",
    "\n",
    "    # Apply the softmax function to the logits\n",
    "    # This transforms the logits into a vector of probabilities, where each element sums to 1\n",
    "    return softmax(logits)"
   ],
   "id": "61a124586fef045",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:51:02.493012Z",
     "start_time": "2025-05-09T12:51:02.484441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final model function\n",
    "def transformer_model(input):\n",
    "\n",
    "    # Embedding\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "\n",
    "    # Attention Mechanism\n",
    "    attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "\n",
    "    # Layer linear and softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "\n",
    "    # Choosing the indices with the highest probability\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "\n",
    "    return output_indices"
   ],
   "id": "84b7368a69ee809c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:51:33.427846Z",
     "start_time": "2025-05-09T12:51:33.348010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generating random data for model input\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "\n",
    "# Making predictions with the model\n",
    "output = transformer_model(input_sequence)\n",
    "\n",
    "print(\"Model Output:\", output)"
   ],
   "id": "58f4d6e56505acc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: [85 60 27 87 46 12  7 61 85 74]\n",
      "Model Output: [36 18 55 70  1 28 62  5 36 22]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e0338be590a7169"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
